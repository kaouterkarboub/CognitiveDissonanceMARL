"""

In this method, we define the transition probabilities for each agent's belief state given the action taken by that agent, and we define the reward function for each agent's belief state and action taken. We assume that if an agent chooses to search, their belief in climate change will be reinforced or weakened depending on the actual state of the world, while if they choose not to search, their belief will remain unchanged. The rewards for each agent are negative, reflecting the cost of searching for information (e.g., time, effort) and the possibility of encountering conflicting information that might create cognitive dissonance. The reward for searching is higher (more negative) than the reward for not searching, to reflect the fact that searching is more costly.


""""

import numpy as np
import random
import matplotlib.pyplot as plt

# Define the Dec-POMDP
class DecPOMDP:
    def __init__(self, num_agents, num_searches, months, num_actions, num_features):
        self.num_agents = num_agents
        self.num_states = (num_searches+1, months, 2, 2)
        self.num_actions = num_actions
        self.months=months
        self.num_features = num_features
        self.beliefs = np.ones((num_agents, self.num_states[0])) / self.num_states[0]
        self.transitions = np.zeros((self.num_states[0], self.num_actions, self.num_states[0], self.num_agents))
        self.rewards = np.zeros((self.num_states[0], self.num_actions, self.num_agents))

    def update_beliefs(self, observations):
        for i in range(self.num_agents):
            self.beliefs[i] = np.dot(self.transitions[:, :, :, i], np.diag(observations)).dot(self.beliefs[i])
            self.beliefs[i] /= np.sum(self.beliefs[i])

    def get_joint_beliefs(self):
        joint_beliefs = np.ones(self.num_states)
        for i in range(self.num_agents):
            joint_beliefs *= self.beliefs[i]
        joint_beliefs /= np.sum(joint_beliefs)
        return joint_beliefs

    def get_reward(self, state, actions):
        joint_beliefs = self.get_joint_beliefs()
        reward = 0
        for i in range(self.num_agents):
            reward += self.rewards[state, actions[i], i] * joint_beliefs[state]
        return reward

    def get_transition(self, state, actions):
        joint_beliefs = self.get_joint_beliefs()
        transition = np.zeros(self.num_states)
        for i in range(self.num_agents):
            transition += self.transitions[:, actions[i], state, i] * joint_beliefs
        return transition
# Define the environment
class Environment:
    def __init__(self, num_agents, num_actions, num_features, months, num_searches):
        self.num_agents = num_agents
        self.num_actions = num_actions
        self.num_features = num_features
        self.months = months
        self.num_searches = num_searches
        self.state = self.reset()

        # Define the transition and reward matrices for the Dec-POMDP
        self.num_states = (num_searches+1, months, 2, 2) # (num_searches, month, belief1, belief2)
        self.transitions = np.zeros((self.num_states[0], self.num_actions, self.num_states[0], self.num_agents))
        self.rewards = np.zeros((self.num_states[0], self.num_actions, self.num_agents))
        self.init_pomdp()

    def reset(self):
        self.state = np.zeros((self.num_agents, self.num_features))
        self.state[:, 0] = self.num_searches
        return self.state.copy()

    def step(self, actions):
        rewards = np.zeros(self.num_agents)
        done = False

        # Update the state based on the actions
        for i in range(self.num_agents):
            if actions[i] == 0:
                self.state[i, 1] += 1
            elif actions[i] == 1:
                self.state[i, 2] += 1

        # Get the current state as a tuple
        state_tuple = tuple(self.state[i] for i in range(self.num_agents))

        # Determine the reward for each agent based on the Dec-POMDP
        reward_model = DecPOMDP(self.num_agents, self.num_states, self.num_actions, self.num_features)
        reward_model.transitions = self.transitions
        reward_model.rewards = self.rewards
        reward_model.update_beliefs(self.state[:, 1])
        for i in range(self.num_agents):
            rewards[i] = reward_model.get_reward(state_tuple, actions)

        # Update the number of searches and check if the episode is done
        self.state[:, 0] -= 1
        if np.all(self.state[:, 0] == 0):
            done = True

        return self.state.copy(), rewards, done

    def init_pomdp(self):
        for i in range(self.num_agents):
            # Define the transition probabilities
            for s in range(self.num_states[0]):
                for a in range(self.num_actions):
                    for sp in range(self.num_states[0]):
                        prob = 0
                        if a == 0:
                            # Probabilities for search action
                            if sp == s:
                                prob = 0.8
                            elif sp == s - 1:
                                prob = 0.2
                        elif a == 1:
                            # Probabilities for not search action
                            if sp == s:
                                prob = 0.7
                            elif sp == s + 1:
                                prob = 0.3
                        self.transitions[s, a, sp, i] = prob

            # Define the reward function
            for s in range(self.num_states[0]):
                for a in range(self.num_actions):
                    if a == 0:
                        # Rewards for search action
                        self.rewards[s, a, i] = -10 * (self.num_searches - s) / self.num_searches
                    elif a == 1:
                        # Rewards for not search action
                        self.rewards[s, a, i] = -2 * (self.num_searches - s) / self.num_searches
