"""

This notebook trains and tests two types of agents, Q-learning and Actor-Critic, on a custom environment called ClimateChangeEnv.

The environment has two agents, each with a 5-dimensional observation space and a 3-dimensional action space.

The code initializes one set of Q-learning agents and one set of Actor-Critic agents, each with two agents to match the environment.

The Q-learning agents are trained for 1000 episodes by repeatedly resetting the environment, taking actions, observing rewards and next states, and updating their Q-tables. After training, the Q-learning agents are tested by repeatedly taking actions according to their learned Q-tables until the environment returns a "done" signal.

The Actor-Critic agents are trained in the same way, but using their policy and value function networks to take actions and update their weights during training. After training, the Actor-Critic agents are tested in the same way as the Q-learning agents.

"""
"""
    def __init__(self, num_agents, n_obs, n_actions, initial_state):
        super().__init__()
        self.n_agents = num_agents
        self.observation_space = Tuple([Discrete(n_obs)] * self.n_agents)
        self.action_space = Tuple([Discrete(n_actions)] * self.n_agents)
        self.initial_state = initial_state
        self.state = self.initial_state

    def reset(self):
        self.states = self.initial_state
        return self.get_obs()

    def step(self, actions):
        # Update the state based on the actions and return the next state, reward, and done flag for each agent
        next_state = update_state(self.state, actions, n_agents, states)
        rewards = compute_rewards(self.state, actions, next_state)
        dones = check_done(next_state)
        self.state = next_state
        return self.get_obs(), rewards, dones, {}
"""


max_timesteps=1000

# Define the environment as a gym wrapper

class ClimateChangeEnv():
    def __init__(self, num_agents, n_obs, n_actions, initial_state, ):
        self.climate_state = np.random.random(10)
        self.group_state = np.random.random(5)
        self.n_agents = num_agents
        self.observation_space = Tuple([Discrete(n_obs)] * self.n_agents)
        self.action_space = Tuple([Discrete(n_actions)] * self.n_agents)
        self.initial_state = initial_state
        self.state = self.initial_state

    def reset(self):
        self.climate_state = np.random.random(10)
        self.group_state = np.random.random(5)
        return (self.climate_state, self.group_state)

    def step(self, actions):
        # Update climate state based on action
        self.climate_state = self.climate_state + np.random.random(10) * actions[0]
        
        # Update group state based on action
        self.group_state = self.group_state + np.random.random(5) * actions[1]
        
        # Calculate reward
        reward = np.sum(self.climate_state) + np.sum(self.group_state)
        
        # Check if episode is done
        done = False
        
        return (self.climate_state, self.group_state), reward, done, {}
  


    def get_obs(self):
        # Get the observation for each agent
        obs = [self.state] * self.n_agents
        return obs

def update_state(state, action, agents, states):
    # Get the current state of the environment
    climate_state, group_state = state
    print(climate_state, group_state)

    # Update the climate state based on the action
    new_climate_state = climate_state + action[0]

    # Update the group state based on the other agents' actions
    new_group_state = []
    for i, agent in enumerate(agents):
        if i == agent.id:
            # Update the state of the current agent
            new_group_state.append(agent.update_state((action[1],)))
        else:
            # Get the action of the other agent
            other_action = agent.get_action(climate_state)
            # Update the state of the other agent
            new_group_state.append(agent.update_state((other_action,)))

    # Return the new state of the environment
    return (new_climate_state, tuple(new_group_state))



def compute_rewards(state, action, next_state, agents):
    # Get the current and next state of the environment
    climate_state, group_state = state
    next_climate_state, next_group_state = next_state

    # Compute the reward for the climate state
    climate_reward = -abs(next_climate_state - target_state)

    # Compute the reward for each agent
    group_rewards = []
    for i, agent in enumerate(agents):
        if i == agent.id:
            # Compute the reward for the current agent based on its action
            group_reward = agent.compute_reward(action, group_state[i], next_group_state[i])
        else:
            # Get the action of the other agent
            other_action = agent.get_action(climate_state)
            # Compute the reward for the other agent based on its action
            group_reward = agent.compute_reward(other_action, group_state[i], next_group_state[i])
        group_rewards.append(group_reward)

    # Return the rewards for the climate state and each agent
    return climate_reward, tuple(group_rewards)


def check_done(state, t):
    # Check if the maximum time step has been reached
    if t >= max_timesteps:
        return True

    # Check if the climate state has reached the target state
    climate_state, _ = state
    if climate_state == target_state:
        return True

    return False



# Define the actor-critic agent
class ActorCriticAgent:
    def __init__(self, alpha=0.01, gamma=0.99, n_hidden=32, n_agents=2):
        self.alpha = alpha # Learning rate
        self.gamma = gamma # Discount factor
        self.n_hidden = n_hidden # Number of hidden units
        self.n_agents = n_agents # Number of agents
        self.actor = self.build_actor() # Actor network
        self.critic = self.build_critic() # Critic network

    def build_actor(self):
        # Define the actor network
        inputs = []
        for i in range(self.n_agents):
            inputs.append(Input(shape=(n_obs,)))
        x = []
        for i in range(self.n_agents):
            x.append(Dense(self.n_hidden, activation='relu')(inputs[i]))
        output_layers = []
        for i in range(self.n_agents):
            output_layers.append(Dense(n_actions, activation='softmax')(x[i]))
        model = Model(inputs=inputs, outputs=output_layers)
        model.compile(optimizer=Adam(learning_rate=self.alpha), loss='categorical_crossentropy')
        return model

    def build_critic(self):
        # Define the critic network
        inputs = []
        for i in range(self.n_agents):
            inputs.append(Input(shape=(n_obs,)))
        x = []
        for i in range(self.n_agents):
            x.append(Dense(self.n_hidden, activation='relu')(inputs[i]))
        concatenated_layer = concatenate(x)
        common_hidden_layer = Dense(self.n_hidden, activation='relu')(concatenated_layer)
        output_layer = Dense(1)(common_hidden_layer)
        model = Model(inputs=inputs, outputs=output_layer)
        model.compile(optimizer=Adam(learning_rate=self.alpha), loss='mse')
        return model

    def act(self, observations):
        # Choose an action based on a joint alpha-greedy policy
        actions = []
        for i in range(self.n_agents):
            if np.random.random() < self.alpha:
                actions.append(np.random.choice(n_actions))
            else:
                agent_observations = [observations[j] if j == i else np.zeros(n_obs) for j in range(self.n_agents)]
                actions.append(np.argmax(self.actor.predict(agent_observations)[i]))
        return actions

    def learn(self, observations, actions, rewards, next_observations, dones):
        # Update the joint action-value function and the critic using the dec-POMDP Bellman equation
        td_targets = []
        for i in range(self.n_agents):
            agent_next_observations = [next_observations[j] if j == i else np.zeros(n_obs) for j in range(self.n_agents)]
            td_target = rewards[i] + self.gamma * self.critic.predict(agent_next_observations)[0] * (1 - dones[i])
            td_targets.append(td_target)
        td_error = td_targets - self.critic.predict(observations)[0]
        self.critic.train_on_batch(observations, td_targets)
        agent_observations = [observations[j] for j in range(self.n_agents)]
        agent_actions = [to_categorical(actions[j], n_actions) for j in range(self.n_agents)]
        agent_advantages = td_error.reshape(-1, 1)
        agent_targets = [agent_advantages * agent_actions[j] for j in range(self.n_agents)]
        self.actor.train_on_batch(agent_observations, agent_targets)
